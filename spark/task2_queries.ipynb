{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"52939e3d-35c7-4767-a59e-ba75f716e952","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.0.1 is available.\r\n","You should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"]}],"source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n","!pip install -q ipython_unittest"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"499e46f5-12a4-46de-a212-90f2e936461e","showTitle":false,"title":""}},"outputs":[],"source":["# Loading modules that we need\n","import unittest\n","from pyspark.sql.dataframe import DataFrame\n","from typing import Any"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7f8c68e0-6bda-40a6-8588-381e51dc0a93","showTitle":false,"title":""}},"outputs":[],"source":["# A helper function to load a table (stored in Parquet format) from DBFS as a Spark DataFrame \n","def load_df(table_name: str) -> DataFrame:\n","    \"\"\"\n","    Arguments:\n","        table_name: name of the table to load\n","    \"\"\"\n","    return spark.read.parquet(table_name)\n","\n","users_df = load_df(\"/user/hive/warehouse/users\")\n","comments_df = load_df(\"/user/hive/warehouse/comments\")\n","posts_df = load_df(\"/user/hive/warehouse/posts\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0a2274f0-1bd8-4812-83d2-f793587e9548","showTitle":false,"title":""}},"source":["#### Subtask 1: implementing two helper functions\n","Impelment these two functions:\n","1. 'run_query' that gets a Spark SQL query and run it on df which is a Spark DataFrame; it returns the content of the first column of the first row of the DataFrame that is the output of the query;\n","2. 'run_query2' that is similar to 'run_query' but instead of one DataFrame gets two; it returns the content of the first column of the first row of the DataFrame that is the output of the query.\n","\n","Note that the result of a Spark SQL query is itself a Spark DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f5fce811-7dd2-4602-a243-18a36754c302","showTitle":false,"title":""}},"outputs":[],"source":["def run_query(query: str, df: DataFrame) -> Any:\n","    \"\"\"\n","    Arguments:\n","        query: a SQL query string\n","        df: the DataFrame that the query will be executed on\n","    \"\"\"\n","    df.createOrReplaceTempView(\"df\")\n","    query_result = spark.sql(query)\n","    return query_result.collect()[0][0]\n","\n","def run_query2(query: str, df1: DataFrame, df2: DataFrame) -> Any:\n","    \"\"\"\n","    Arguments:\n","        query: a SQL query string\n","        df1: DataFrame A\n","        df2: DataFrame B\n","    \"\"\"\n","    df1.createOrReplaceTempView(\"df1\")\n","    df2.createOrReplaceTempView(\"df2\")\n","    query_result = spark.sql(query)\n","    return query_result.collect()[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2a021cce-880b-42b1-a09d-6d2c2222124e","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["The ipython_unittest extension is already loaded. To reload it, use:\n","  %reload_ext ipython_unittest\n"]}],"source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n","%load_ext ipython_unittest"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7de10aba-7e77-4baa-af35-2b4e37916071","showTitle":false,"title":""}},"source":["#### Subtask 2: writing a few queries\n","Write the following queries in SQL to be executed by Spark in the next cell.\n","\n","1. 'q1': find the 'Id' of the most recently created post ('df' is 'posts_df') \n","2. 'q2': find the number users\n","3. 'q3': find the 'Id' of the user who posted most number of answers\n","4. 'q4': find the number of questions\n","5. 'q5': find the display name of the user who posted most number of comments\n","\n","Note that 'q1' is already available below as an example. Moreover, remmebr that Spark supports ANSI SQL 2003 so your queries have to comply with that standard."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c3273ef7-7f4d-4b5d-bcf7-9935c952e26d","showTitle":false,"title":""}},"outputs":[],"source":["# Find the 'Id' of the most recently created post\n","# 'df' is 'posts_df'\n","q1 = \"SELECT * FROM df ORDER BY CreationDate DESC limit 1\"\n","\n","# Find the number of users\n","# 'df' is 'users_df'\n","q2 = \"SELECT COUNT(*) FROM df\"\n","\n","# Find the 'Id' of the user who posted most number of answers\n","# 'df' is 'posts_df'\n","# 'PostTypeId = 2' is an answer post\n","q3 = \"\"\"\n","    SELECT OwnerUserId, COUNT(Id)\n","    FROM df\n","    WHERE PostTypeId = 2\n","    GROUP BY OwnerUserId\n","    ORDER BY COUNT(Id) DESC\n","    LIMIT 1\n","\"\"\"\n","\n","# Find the number of questions\n","# 'df' is 'posts_df'\n","q4 = \"\"\"\n","    SELECT COUNT(Id)\n","    FROM df\n","    WHERE PostTypeId = 1\n","\"\"\"\n","\n","# Find the display name of the user who posted most number of comments\n","# 'df1' is 'users_df'\n","# 'df2' is 'comments_df'\n","q5 = \"\"\"\n","    SELECT DisplayName\n","    FROM df1\n","    WHERE Id = (\n","      SELECT UserId\n","      FROM df2\n","      GROUP BY df2.UserId\n","      ORDER BY COUNT(*) DESC\n","      LIMIT 1\n","    );\n","\"\"\""]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"46400bac-c11c-4bee-81a8-67d42e3ca968","showTitle":false,"title":""}},"source":["#### Subtask 3: validating the implementations by running the tests\n","\n","Run the cell below and make sure that all the tests run successfully."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"31ca3fb9-427c-425d-b334-0df9c208a21b","showTitle":false,"title":""}},"outputs":[{"data":{"application/unittest.status+json":{"color":"yellow","message":"","previous":0},"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/unittest.status+json":{"color":"yellow","message":"","previous":0},"text/plain":""},"datasetInfos":[],"executionCount":null,"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/unittest.status+json":{"color":"lightgreen","message":".....\n----------------------------------------------------------------------\nRan 5 tests in 7.437s\n\nOK\n","previous":0},"text/plain":["Success"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/unittest.status+json":{"color":"lightgreen","message":".....\n----------------------------------------------------------------------\nRan 5 tests in 7.437s\n\nOK\n","previous":0},"text/plain":"Success"},"datasetInfos":[],"executionCount":null,"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[".....\n","----------------------------------------------------------------------\n","Ran 5 tests in 7.437s\n","\n","OK\n","Out[26]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]}],"source":["%%unittest_main\n","class TestTask2(unittest.TestCase):\n","    def test_q1(self):\n","        # find the id of the most recent post\n","        r = run_query(q1, posts_df)\n","        self.assertEqual(r, 95045)\n","\n","    def test_q2(self):\n","        # find the number of the users\n","        r = run_query(q2, users_df)\n","        self.assertEqual(r, 91616)\n","\n","    def test_q3(self):\n","        # find the user id of the user who posted most number of answers\n","        r = run_query(q3, posts_df)\n","        self.assertEqual(r, 64377)\n","\n","    def test_q4(self):\n","        # find the number of questions\n","        r = run_query(q4, posts_df)\n","        self.assertEqual(r, 28950)\n","\n","    def test_q5(self):\n","        # find the display name of the user who posted most number of comments\n","        r = run_query2(q5, users_df, comments_df)\n","        self.assertEqual(r, \"Neil Slater\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fe8a2e0c-7e72-410b-b385-00503c0bc136","showTitle":false,"title":""}},"source":["#### Subtask 4: answering to questions about Spark related concepts\n","\n","Please answer the following questions. Write your answer in one to two short paragraphs. Don't copy-paste; instead, write your own understanding.\n","\n","1. What is the difference between 'DataFrame', 'Dataset', and 'Resilient Distributed Datasets (RDD)'? \n","2. When do you suggest using RDDs instead of using DataFrames?\n","3. What is the main benefit of using DataSets instead of DataFrames?\n","\n","Write your answers in the next cell."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9c33ca21-1672-4c0c-a876-f73cbb8f65b2","showTitle":false,"title":""}},"source":["#### 1. DataFrames, Datasets and Resilient Distributed Datasets (RDDs)\n","\n","The key difference between RDDs on one hand, and DataFrames/Datasets on the other hand, is that an RDD treats its data as **unstructured**, whereas a DataFrame/Dataset has a **schema** that defines **named columns** for its data. Datasets are essentially a more **type-safe** version of DataFrames. A `Dataset<T>` is a dataset with elements of type `T`, and in Spark 2.0, a DataFrame is an alias for `Dataset<Row>`, where `Row` is an untyped object (though still with a defined schema at runtime).\n","\n","The defined schema of DataFrames and Datasets allow the use of **Spark SQL**, so users may write SQL queries instead of using Spark methods. The Spark SQL engine also includes an **optimizer**, which can drastically improve performance of operations on these DataFrames/Datasets.\n","\n","#### 2. When to use RDDs instead of DataFrames\n","\n","RDDs are preferable to DataFrames when the **schema of the data is unknown**. In this sense, RDDs may be more flexible, as they do not require the user to define the schema of the data. RDDs should likely not be used instead of DataFrames if the data _has_ a known schema and the system can benefit from the improved performance of DataFrames' optimizer.\n","\n","#### 3. Benefits of using Datasets instead of DataFrames\n","\n","Datasets provide better **compile-time type safety** than DataFrames. The generic type `T` of `Dataset<T>` (as described in answer 1) allows the compiler to check that the Dataset API is used correctly according to the data type. This obviously only works for statically typed compiled languages – in Python, DataFrames are used where Scala/Java could use Datasets, since Python does not have built-in compile-time type checking."]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Task2","notebookOrigID":346926439992861,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
