{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4d813ca3-8c88-4f54-b629-95f7aa3eff2c","showTitle":false,"title":""}},"outputs":[],"source":["from typing import Final, Literal, Optional, Callable, Union, List"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"324eb902-51f2-44b0-8ff1-730efac9900c","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Out[80]: True"]}],"source":["# Deleting tables left from previous runs in case they still exist after deleting an inactive cluster\n","dbutils.fs.rm(\"/user\", recurse=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"60b530d6-b580-4de2-affb-aad878f4da86","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: You are using pip version 21.0.1; however, version 23.0.1 is available.\r\n","You should consider upgrading via the '/databricks/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"]}],"source":["# We need to install 'ipython_unittest' to run unittests in a Jupyter notebook\n","!pip install -q ipython_unittest"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"023f4d10-e729-450c-8d1a-50d494b488d6","showTitle":false,"title":""}},"outputs":[],"source":["# Loading PySpark modules that we need\n","import unittest\n","from collections import Counter\n","from pyspark.sql import DataFrame, Column\n","from pyspark.sql.types import *"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fcda919c-b51e-4b61-9d61-80cc24f2d15e","showTitle":false,"title":""}},"source":["#### Subtask 1: defining the schema for the data\n","Typically, the first thing to do before loading the data into a Spark cluster is to define the schema for the data. Look at the schema for 'badges' and try to define the schema for other tables similarly."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"124debcf-7c98-420d-96a3-dded00514335","showTitle":false,"title":""}},"source":["[Data Types Documentation](https://spark.apache.org/docs/3.3.1/api/python/reference/pyspark.sql/data_types.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ab8bdba5-f6d6-43bf-bed9-763d99cfcc91","showTitle":false,"title":""}},"outputs":[],"source":["# Defining a schema for 'badges' table\n","badges_schema = StructType([\n","    StructField('UserId', IntegerType(), False),\n","    StructField('Name', StringType(), False),\n","    StructField('Date', TimestampType(), False),\n","    StructField('Class', IntegerType(), False)\n","])\n","\n","# Defining a schema for 'posts' table\n","posts_schema = StructType([\n","    StructField(\"Id\", IntegerType(), False),\n","    StructField(\"ParentId\", IntegerType(), False),\n","    StructField(\"PostTypeId\", IntegerType(), False),\n","    StructField(\"CreationDate\", TimestampType(), False),\n","    StructField(\"Score\", IntegerType(), False),\n","    StructField(\"ViewCount\", IntegerType(), False),\n","    StructField(\"Body\", StringType(), False), \n","    StructField(\"OwnerUserId\", IntegerType(), False),\n","    StructField(\"LastActivityDate\", TimestampType(), False),\n","    StructField(\"Title\", StringType(), False),\n","    StructField(\"Tags\", StringType(), False), # transform to ArrayType(StringType()) later\n","    StructField(\"AnswerCount\", IntegerType(), False),\n","    StructField(\"CommentCount\", IntegerType(), False),\n","    StructField(\"FavoriteCount\", IntegerType(), False),\n","    StructField(\"CloseDate\", TimestampType(), False)\n","])\n","\n","# Defining a schema for 'users' table\n","users_schema = StructType([\n","    StructField(\"Id\", IntegerType(), False),\n","    StructField(\"Reputation\", IntegerType(), False),\n","    StructField(\"CreationDate\", TimestampType(), False),\n","    StructField(\"DisplayName\", StringType(), False),\n","    StructField(\"LastAccessDate\", TimestampType(), False),\n","    StructField(\"AboutMe\", StringType(), False),\n","    StructField(\"Views\", IntegerType(), False),\n","    StructField(\"UpVotes\", IntegerType(), False),\n","    StructField(\"DownVotes\", IntegerType(), False),\n","])\n","\n","# Defining a schema for 'comments' table\n","comments_schema = StructType([\n","    StructField(\"PostId\", IntegerType(), False),\n","    StructField(\"Score\", IntegerType(), False),\n","    StructField(\"Text\", StringType(), False),\n","    StructField(\"CreationDate\", TimestampType(), False),\n","    StructField(\"UserId\", IntegerType(), False)\n","])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fab2cfe2-0961-4a22-8fb1-9a6f9191fbcf","showTitle":false,"title":""}},"source":["#### Subtask 2: implementing two helper functions\n","Next, we need to implement two helper functions:\n","1. 'load_csv' that as input argument receives path for a CSV file and a schema and loads the CSV pointed by the path into a Spark DataFrame and returns the DataFrame;\n","2. 'save_df' receives a Spark DataFrame and saves it as a Parquet file on DBFS.\n","\n","Note that the column separator in CSV files is TAB character ('\\t') and the first row includes the name of the columns. \n","\n","BTW, DBFS is the name of the distributed filesystem used by Databricks Community Edition to store and access data."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"be6dff57-e402-49f0-a6fc-273283511f6a","showTitle":false,"title":""}},"source":["[Data Sources Documentation](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"089f87ff-f2c8-4ac8-8449-cec251c502f6","showTitle":false,"title":""}},"outputs":[],"source":["TABLE_ROOT_PATH: Final[Literal[\"/FileStore/tables/\"]] = \"/FileStore/tables\"\n","PARQUET_ROOT_PATH: Final[Literal[\"/user/hive/warehouse\"]] = \"/user/hive/warehouse\"\n","\n","def load_csv(source_file: str, schema: StructType) -> DataFrame:\n","    \"\"\"\n","    Arguments:\n","        source_file: path for the CSV file to load\n","        schema: schema for the CSV file being loaded as a DataFrame\n","        post_read: an optional callable that takes in the DataFrame as a parameter, and returns a DataFrame\n","    \"\"\"\n","    df = spark.read.option(\"delimiter\", \"\\t\").option(\"header\", True).schema(schema).csv(source_file)\n","    return df\n","\n","def save_df(df: DataFrame, table_name: str) -> None:\n","    \"\"\"\n","    Arguments:\n","        df: DataFrame to be saved\n","        table_name: name under which the DataFrame will be saved\n","    \"\"\"\n","    df.write.save(f\"{PARQUET_ROOT_PATH}/{table_name}\", format=\"parquet\", mode=\"overwrite\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"39bc683c-b37a-4842-8bf8-004620b17cca","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["The ipython_unittest extension is already loaded. To reload it, use:\n","  %reload_ext ipython_unittest\n"]}],"source":["# Loading 'ipython_unittest' so we can use '%%unittest_main' magic command\n","%load_ext ipython_unittest"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8903e903-7e4f-4c15-99bd-c9129a601fde","showTitle":false,"title":""}},"source":["#### Subtask 3: validating the implementation by running the tests\n","\n","Run the cell below and make sure that all the tests run successfully. Moreover, at the end there should be four Parquet files named 'badges', 'comments', 'posts', and 'users' in '/user/hive/warehouse'.\n","\n","Note that we assumed that the data for the project has already been stored on DBFS on the '/FileStore/tables/' path. (I mean as 'badges_csv.gz', 'comments_csv.gz', 'posts_csv.gz', and 'users_csv.gz'.)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cd470d59-2571-4b7d-b022-9ee8f7c3e281","showTitle":false,"title":""}},"outputs":[{"data":{"application/unittest.status+json":{"color":"yellow","message":"","previous":0},"text/plain":[]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/unittest.status+json":{"color":"yellow","message":"","previous":0},"text/plain":""},"datasetInfos":[],"executionCount":null,"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"data":{"application/unittest.status+json":{"color":"lightgreen","message":".....\n----------------------------------------------------------------------\nRan 5 tests in 22.952s\n\nOK\n","previous":0},"text/plain":["Success"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":{"application/unittest.status+json":{"color":"lightgreen","message":".....\n----------------------------------------------------------------------\nRan 5 tests in 22.952s\n\nOK\n","previous":0},"text/plain":"Success"},"datasetInfos":[],"executionCount":null,"metadata":{},"removedWidgets":[],"type":"mimeBundle"}},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":[".....\n","----------------------------------------------------------------------\n","Ran 5 tests in 22.952s\n","\n","OK\n","Out[86]: <unittest.runner.TextTestResult run=5 errors=0 failures=0>"]}],"source":["%%unittest_main\n","class TestTask1(unittest.TestCase):\n","   \n","    # test 1\n","    def test_load_badges(self):\n","        # changed file path to badges.csv\n","        result = load_csv(source_file=\"/FileStore/tables/badges.csv\", schema=badges_schema)\n","        self.assertIsNotNone(result, \"Badges dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 105640, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower, ['UserId', 'Name', 'Date', 'Class']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 2\n","    def test_load_posts(self):\n","        # changed file path to posts.csv\n","        result = load_csv(source_file=\"/FileStore/tables/posts.csv\", schema=posts_schema)\n","        self.assertIsNotNone(result, \"Posts dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 61432, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower,\n","                                   ['Id', 'ParentId', 'PostTypeId', 'CreationDate', 'Score', 'ViewCount', 'Body', 'OwnerUserId',\n","                                    'LastActivityDate', 'Title', 'Tags', 'AnswerCount', 'CommentCount', 'FavoriteCount',\n","                                    'CloseDate']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 3\n","    def test_load_comments(self):\n","        # changed file path to comments.csv\n","        result = load_csv(source_file=\"/FileStore/tables/comments.csv\", schema=comments_schema)\n","        self.assertIsNotNone(result, \"Comments dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 58735, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower, ['PostId', 'Score', 'Text', 'CreationDate', 'UserId']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    \n","    # test 4\n","    def test_load_users(self):\n","        # changed file path to users.csv\n","        result = load_csv(source_file=\"/FileStore/tables/users.csv\", schema=users_schema)\n","        self.assertIsNotNone(result, \"Users dataframe did not load successfully\")\n","        self.assertIsInstance(result, DataFrame, \"Result type is not of spark.sql.DataFrame\")\n","        self.assertEqual(result.count(), 91616, \"Number of records is not correct\")\n","\n","        coulmn_names = Counter(map(str.lower,\n","                                   ['Id', 'Reputation', 'CreationDate', 'DisplayName', 'LastAccessDate', 'AboutMe',\n","                                    'Views', 'UpVotes', 'DownVotes']))\n","        self.assertCountEqual(coulmn_names, Counter(map(str.lower, result.columns)),\n","                              \"Missing column(s) or column name mismatch\")\n","    # test 5\n","    def test_save_dfs(self):\n","        dfs = [(\"/FileStore/tables/users.csv\", users_schema, \"users\"),\n","               (\"/FileStore/tables/badges.csv\", badges_schema, \"badges\"),\n","               (\"/FileStore/tables/comments.csv\", comments_schema, \"comments\"),\n","               (\"/FileStore/tables/posts.csv\", posts_schema, \"posts\")\n","               ]\n","\n","        for i in dfs:\n","            df = load_csv(source_file=i[0], schema=i[1])\n","            save_df(df, i[2])"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a415500d-d01a-4ffb-b708-e35cbb69129f","showTitle":false,"title":""}},"source":["##### Conversions, decoding, and post-processing prior to saving\n","Some of the columns need additional processing prior to being used in task 2. The following processing steps are performed here, and the resulting tables are saved.\n","We weren't quite sure where to put this, so we just put it here 😇\n","###### Posts\n","- Convert the `Tags` column from a string to an array of strings\n","- Base64 decode `Body` and `Title`\n","###### Comments\n","- Base64 decode `Text`"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"1723f2f3-c8a7-41f1-ae3c-e289b259b703","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import split, col, transform, regexp_replace, udf, unbase64\n","\n","@udf(returnType=ArrayType(StringType()))\n","def to_array(string: Optional[str]) -> List[str]:\n","  \"\"\"\n","  Convert a string of format <item-1><item-2><item-3> to a list\n","  [\"list-1\", \"list-2\", \"list-3\"]\n","  \"\"\"\n","  if string is not None:\n","    return string.strip(\"<>\").split(\"><\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6026ecb8-b950-43bb-80be-c8a66f35240e","showTitle":false,"title":""}},"outputs":[],"source":["# Handle the tag array fields after loading the CSVs\n","posts_df = load_csv(source_file=\"/FileStore/tables/posts.csv\", schema=posts_schema)\n","posts_df = posts_df.withColumn(\"Tags\", to_array(\"Tags\"))\n","posts_df = posts_df.withColumn(\"Body\", unbase64(\"Body\").cast(\"string\"))\n","posts_df = posts_df.withColumn(\"Title\", unbase64(\"Title\").cast(\"string\"))\n","save_df(posts_df, \"posts\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6453c18d-d4b2-4505-a3bc-6adfa1984c8b","showTitle":false,"title":""}},"outputs":[],"source":["# Handle the tag array fields after loading the CSVs\n","comments_df = load_csv(source_file=\"/FileStore/tables/comments.csv\", schema=comments_schema)\n","comments_df = comments_df.withColumn(\"Text\", unbase64(\"Text\").cast(\"string\"))\n","save_df(comments_df, \"comments\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0f99b257-8618-4796-aeb0-d9446863c259","showTitle":false,"title":""}},"source":["#### Subtask 4: answering to questions about Spark related concepts\n","\n","Please write a short description for the terms below---one to two short paragraphs for each term. Don't copy-paste; instead, write your own understanding.\n","\n","1. What do the terms 'Spark Application', 'SparkSession', 'Transformations', 'Action', and 'Lazy Evaluation' mean in the context of Spark?\n","\n","Write your descriptions in the next cell."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"91ec9fda-7848-4f01-8a36-3b82b78be007","showTitle":false,"title":""}},"source":["#### Spark Application\n","\n","A **Spark application** is a user-defined program that uses Spark, typically in the form of JAR files (from Scala/Java) or Python files. These files are **submitted** to run the application in a **cluster** of nodes. At runtime, an application consists of a **driver**, which coordinates the program, and **executors**, which run computations on data. There are two main deployment modes:\n","- **Client mode** runs the driver locally, allowing for interacting with the cluster through a Spark shell\n","- **Cluster mode** deploys the driver in the cluster - typically preferred when shell interaction is not required, to minimize network latency between drivers and executors\n","\n","In both of these modes, the driver assigns work to executors, which run on **worker nodes** in the cluster. They all communicate with a **cluster manager** to manage resources.\n","\n","#### SparkSession\n","\n","**SparkSession** is essentially the entry point to Spark's API in an application. It allows users to create data structures like **Datasets** and **DataFrames**, and use **Spark SQL**. It also contains a **SparkContext** - in the deployment model described above, it represents the connection to the cluster. This provides an abstraction layer to the user, giving the power of the cluster through a relatively simple API.\n","\n","#### Resilient Distributed Datasets (extra)\n","\n","In order to properly explain transformations and actions below, we felt the need to first explain **Resilient Distributed Datasets (RDDS)**. An RDD is an immutable, patitioned collection of objects, which may derive from several different data sources - most typical is a data store such as HDFS or a database. It is a central data structure that Spark operates on.\n","\n","The **resilient** part of the name comes from the fact that a partition of an RDD that is corrupted or otherwise lost can be recovered by \"replaying\" the transformations (explained below) on the datasets it derives from. It keeps a history of the transformations to apply to it, called **lineage**.\n","\n","#### Transformations\n","\n","A **transformation** is an operation on an RDD that produces a *new, transformed* RDD. Examples of transformations are `map`, `filter` and `flatMap`, all of which take a function as an argument, and applies it on every element of the RDD to transform it.\n","\n","There are also transformations that work on 2 RDDs to **combine** them into a transformed RDD. Examples of this are `union`, `intersection` and `subtract`, all of which operate on one RDD and take another RDD as an argument. In addition, there are transformations that work specifically for RDDs of `(key, value)` pairs, such as `reduceByKey`, `groupByKey` and `mapValues`. Finally, there are transformations that work on 2 `(key, value)`-pair RDDs, such as `subtractByKey` and `join`.\n","\n","#### Action\n","\n","An **action** is an operation on an RDD that produces a **result**. This result can vary in type depending on the action, but the key difference from a transformation is that the result is not a new RDD. Examples of actions are `collect`, `count` and `reduce`.\n","\n","Similarly to transformations, there are also actions that work specifically on `(key, value)` pairs. Examples of this are `countByKey`, `collectAsMap` and `lookup`.\n","\n","#### Lazy Evaluation\n","\n","**Lazy evaluation** in the context of Spark refers to how transformations and actions are evaluated. Since an RDD doesn't really \"matter\" until it is used for some result, the evaluation of _transformations_ on the RDD can be delayed until an _action_ is called on it. This is what is meant by \"lazy\": postponing actually _doing_ the transformation until it is needed. This is similar to the concept of **iterators** in many programming languages."]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"Task1","notebookOrigID":346926439992829,"widgets":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}
